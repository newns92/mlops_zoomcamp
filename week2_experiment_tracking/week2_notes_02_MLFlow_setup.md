## MLFlow Setup
- https://mlflow.org/docs/latest/index.html
- **MLflow** = open-source platform for managing the end-to-end ML lifecycle that tackles 4 primary functions:
    - **Tracking** = Tracking experiments to record and compare parameters and results
        - an API and UI for logging parameters, code versions, metrics, and artifacts when running ML code and for later visualizing results
        - Can use MLflow Tracking in any environment (Ex: a standalone script or a notebook) to log results to local files or to a server, then compare multiple runs
        - Teams can also use it to compare results from different users
    - **Projects** = Packaging ML code in a reusable, reproducible form in order to share with other data scientists or transfer to production
        - Standard format for packaging reusable data science code
        - Each project is simply a directory with code (or a Git repository) that uses a descriptor file, or simply "convention", to specify its dependencies and how to run the code
            - Ex: Projects can contain a `conda.yaml` file for specifying a Python Conda environment
        - When you use the MLflow Tracking API in a Project, MLflow automatically remembers the project version (ex: Git commit) and any parameters
        - Can easily run existing MLflow Projects from GitHub or your own Git repo, and chain them into multi-step workflows
    - **Models** = Managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms
        - Offers a convention for packaging ML models in multiple "flavors", and a variety of tools to help deploy them
        - Each Model is saved as a directory containing arbitrary files and a descriptor file that lists several “flavors” the model can be used in 
            - Ex: a TensorFlow model can be loaded as a TensorFlow DAG, *or* as a Python function to apply to input data
        - MLflow provides tools to deploy many common model types to diverse platforms
            - Ex: Any model supporting the “Python function” flavor can be deployed to a Docker-based REST server, to cloud platforms such as Azure ML and AWS SageMaker, and as a user-defined function in Apache Spark for batch and streaming inference
         If you output MLflow Models using the Tracking API, MLflow also automatically remembers which Project and run they came from
    - **Model Registry** = Providing a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations
        - Offers a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model
        - Provides model lineage (which MLflow experiment and run produced the model), model versioning, stage transitions (Ex: from staging to production or archiving), and annotations
- Can use it with any ML library, and in any programming language, since all functions are accessible through a REST API and CLI
- Create a new (or activate a current) Anaconda environment named `zoom`
    - *If on the VM, create a new environment as well (if need be)*
- Install the following Python packages, if need be (either manually or via a `requirements.txt` with `pip install -r requirements.txt`):
    - `mlflow`
    - `jupyter`
    - `scikit-learn`
    - `pandas`
    - `seaborn`
    - `hyperopt` (Distributed asynchronous hyperparameter optimization: https://hyperopt.github.io/hyperopt/)
    - `xgboost` (optimized distributed gradient boosting library that implements ML algorithms under the Gradient Boosting framework: https://xgboost.readthedocs.io/en/stable/)
    - `fastparquet` (Python implementation of the parquet format, aiming to integrate into Python-based big data work-flows: https://pypi.org/project/fastparquet/)
    - `boto3` (use the AWS SDK for Python (Boto3) to create, configure, and manage AWS services: https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    - **NOTE**: If you get an error about `py4j` and PySpark, install these all in a new environment for this course
        - *Or* downgrade it via: `pip install --force-reinstall -v "py4j==0.10.9.5"`
- Run `mlflow --version` and check the output (currently version 2.3.2)

### Launching the MLFlow UI and Running MLFlow
- In the Anaconda environment, run `mlflow ui --backend-store-uri sqlite:///mlflow.db`
    - This tells MLFlow that we want to store all the artifacts and metadata in SQLite
- You will get output that says the UI is being served at `http://127.0.0.1:5000`, which you can open in a web browser
- Import `mlflow` in the new version of the notebook, `duration-prediction_MLFlow.ipynb`
- Once we set the expirement `nyc_taxi_experiment_1`, reload the browser window of the UI to see it
- Once we do an MLFlow run in the notebook, we can see it in the browser once it's refreshed
    - We can see duration of the run, user (from the machine/computer), source, detected code version, models (if logged), metrics (if logged)
    - Clicking on the run, we can see all of the above as well as the tag(s), metric(s) and the parameter(s) we logged

### Hyperparameter Tuning and Logging
- After the first run of the `xgboost` cell, we will start to see of our experiment runs in the MLFlow UI
- On the `http://127.0.0.1:5000/#/experiments/` page, we can filter/search for components of experiments
    - Like tags via searching for ```tags.`model` = 'xgboost'```
    - We can select multiple runs and then click "Compare" to compare them via a dashboard
    - In the "Parallel Coordinates Plot", we can see how various hyperparameter values affected our metric (RMSE)
        - Can even highlight specific metric values by clicking on the far-right y-axis labeled after the metric
    - In the "Scatter Plot", we can plot a metric by a specific hyperparameter
        - For example, look at `rmse` by `min_child_weight` to see a pattern emerge
    - In the "Contour Plot", we set "reverse color" to "On", since we're minimizing our error in this case
        - Here, we can visualize our metric for numerous hyperparameters and their values (say, `min_child_weight` and `learning_rate`)
- Once all runs of the experiment are complete, we can sort the results by our metric's column in the "Experiments" page of the UI
    - Here, we can grab the "best" hyperparameter values that we obtained
    - **Also be sure to note the training time of this run**
        - *Maybe we'd want a lower-complexity model with slightly worse error* (depends on various factors)
- Can enable **autologging** via `mlflow.xgboost.autolog(disable=False)` before setting off an MLFlow run
    - This gives a (potentially) more complete set of parameters, metrics, and artifacts (such as the model, a YAML file, a requirements text file, and feature importance files)
    - Everything will be saved in the local `./mlruns/` directory

### Model Management
- https://neptune.ai/blog/ml-experiment-tracking
- **MLOps Lifecycle:**
    - 1\. Data Sourcing
    - 2\. Data Labeling
    - 3\. Data Versioning
    - 4\. **Model Management**
        - a\. **Experiment Tracking (cyclical)**
            - i\. Model Architecture
            - ii\. Model Training 
            - iii\. Model Evaluation (*back to Model Architecture if needed (cyclical)*)
        - b\. Model Versioning
        - c\. Model Deployment (*potentially back to Data Labeling Stage*)
            - i.\ Scaling Hardware (*potentially going back to Experiment Tracking stage(s)*)
    - 5\. Prediction Monitoring
- ML model management (*a subset of Experiment Tracking*) starts *when models go to production*:
    - Streamlines moving models from experimentation to production
    - Helps with model versioning
    - Organizes model artifacts in an ML model registry
    - Helps with testing various model versions in PROD
    - Enables rolling back to an old model version if the new one seems to be going crazy
- Saving models to file system directories (as `model_v1`, `model_v2`, `final_model`, etc.) is bad practice
    - Error prone
    - No versioning
    - No standardization
    - No clear model lineage (of models, datasets, hyperparameters, etc.)
- We can do this in MLFlow in the `duration-prediction_MLFlow_model_management.ipynb` notebook
- We can log/save models in 2 ways:
    - 1\. As just another artifact via `mlflow.log_artifact('model_name', artifact_path='models/')`
    - 2\. Using the `.log_model()` method via `mlflow.<framework>.log_model(model, artifact_path='models/')`
        - Using this method, we can save models from various frameworks (TensorFlow, PyTorch, Keras, XGBoost, scikit-learn, spaCy, LightGBM, etc.) as an MLFlow Model object
        - We can then access this model in different "flavors" (say, one time as a Python function, another time as a scikit-learn model)
        - We can deploy these retrived/accessed models in various manners (Python function, Docker container, Kubernetes cluster, Spark batch job, SageMaker, Azure, etc.)
    - The "Artifacts" section of the run has some code examples of how to use the logged model to make predictions on Spark and Pandas DataFrames
        - You "load" the model by passing in the **model URI** provided
        - Also, saved models have an `artifact_path` which indicates where the models lives, as well as different methods to *load* it (`python_function`, `xgboost` object, etc.)