{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6c0496",
   "metadata": {},
   "source": [
    "## Data\n",
    "- We will be using NYC taxi data\n",
    "- These have recently been changed to **parquet** files\n",
    "- We will be using Pandas to read this data in via `pd.read_parquet()` command\n",
    "    - This requires instaling PyArrow via `pip install pyarrow` on the VM\n",
    "    - Also potentiall have to `pip install seaborn` and `pip install scikit-learn`\n",
    "- In the `week1/` directory, run `mkdir data`, then `cd` into it \n",
    "- Download the Green January and February 2021 parquet data file via `wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet` and `wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-02.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57e33f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import mlflow\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials  # metrics\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "from config import mlflow_model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7009801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/nimz/Documents/mlops_zoomcamp/week2_experiment_tracking/mlruns/1', creation_time=1684358646144, experiment_id='1', last_update_time=1684358646144, lifecycle_stage='active', name='nyc_taxi_experiment_1', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the MLFlow URI to our backend\n",
    "mlflow.set_tracking_uri('sqlite:///mlflow.db')\n",
    "\n",
    "# set up to assign/append runs to our experiment (and create if it doesn't exist)\n",
    "mlflow.set_experiment('nyc_taxi_experiment_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f3ea8",
   "metadata": {},
   "source": [
    "## 1. Load, inspect, and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbec7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create helper function to read and clean data\n",
    "def read_dataframe(filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # do some data conversion if CSV\n",
    "        df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "        df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "        \n",
    "    elif filename.endswith('.parquet'):\n",
    "        df = pd.read_parquet(filename)\n",
    "    \n",
    "    # create duration in minutes column\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "    \n",
    "    # filter to only trips between 1 minute and 1 hour\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "    \n",
    "    # specify categorical input features and convert to String\n",
    "    # for one-hot encoding via Dictionary Vectorizer\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b310ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73908, 61921)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create DataFrames for training and validation\n",
    "df_train = read_dataframe('./data/green_tripdata_2021-01.parquet')\n",
    "df_val = read_dataframe('./data/green_tripdata_2021-02.parquet')\n",
    "\n",
    "# see how many samples we have\n",
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2ceab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature combining pickup and drop-off locations (feature engineering)\n",
    "df_train['PU_DO'] = df_train['PULocationID'] + '_' + df_train['DOLocationID']\n",
    "df_val['PU_DO'] = df_val['PULocationID'] + '_' + df_val['DOLocationID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f5bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify our input features\n",
    "categorical = ['PU_DO'] #'PULocationID', 'DOLocationID']\n",
    "numerical = ['trip_distance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2870b",
   "metadata": {},
   "source": [
    "## 2. Create training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f72d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation input feature sets\n",
    "dv = DictVectorizer()\n",
    "\n",
    "# turn each row into dictionary and create the training set\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "# turn each row into dictionary and create the validation set\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca9b9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation label sets\n",
    "target = 'duration'\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda20451",
   "metadata": {},
   "source": [
    "## 3. MLFLow Model Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f444ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for XGBoost\n",
    "# DMatrix = internal XGBoost data structure optimized for both memory efficiency and training speed\n",
    "# https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.DMatrix\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "valid = xgb.DMatrix(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d009b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(params):\n",
    "#     '''\n",
    "#     Defines an objective function that will be minimized given the provided hyperparameters\n",
    "#     '''\n",
    "#     with mlflow.start_run():\n",
    "#         # set our tages and log our parameters (search space)\n",
    "#         mlflow.set_tag(\"model\", \"xgboost\")\n",
    "#         mlflow.log_params(params)\n",
    "        \n",
    "#         # train and generate our model\n",
    "#         booster = xgb.train(\n",
    "#             params=params,\n",
    "#             dtrain=train,\n",
    "#             num_boost_round=1000,  # iterations of the booster\n",
    "#             evals=[(valid, 'validation')],  # xgboost tries to minimze error here on this set\n",
    "#             early_stopping_rounds=50,   # stop if 50+ iterations go by with no improvement\n",
    "#             verbose_eval=False  # suppress output\n",
    "#         )\n",
    "        \n",
    "#         # get predictions\n",
    "#         y_pred = booster.predict(valid)\n",
    "        \n",
    "#         # get error and log it\n",
    "#         rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "#         # print(f'Validation RMSE: {rmse}')\n",
    "#         mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "#     # return the error to `hyperopt` and send an alert/signal that\n",
    "#     #   the objective function has succesfully run\n",
    "#     return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "# # define the search space (our hyperparameters and various values) using `hp` ranges\n",
    "# # https://hyperopt.github.io/hyperopt/getting-started/search_spaces/\n",
    "# search_space = {\n",
    "#     'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),  # define an interger range with `scope`\n",
    "#     'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "#     'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "#     'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "#     'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "#     'objective': 'reg:linear',\n",
    "#     'seed': 42\n",
    "# }\n",
    "\n",
    "# # minimize the defined objective function over the defined search space\n",
    "# # https://hyperopt.github.io/hyperopt/getting-started/minimizing_functions/\n",
    "# # i.e., find the best hyperparemeters\n",
    "# # i.e., explore a given function over a hyperparameter space according to a given algorithm, \n",
    "# #   allowing up to a certain number of function evaluations\n",
    "# best_result = fmin(\n",
    "#     fn=objective,\n",
    "#     space=search_space,\n",
    "#     algo=tpe.suggest,  # Given previous trials + the domain, suggest best expected hp point according to TPE-EI algorithm\n",
    "#     max_evals=50,\n",
    "#     trials=Trials()  # keep track of information from each run\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aea4e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = {\n",
    "    'learning_rate': 0.13642747972651512,\n",
    "    'max_depth': 21.0,\n",
    "    'min_child_weight': 1.5655550191042376,\n",
    "    'reg_alpha': 0.009403472263570046,\n",
    "    'reg_lambda': 0.005322134643445022\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5da74458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off autologging\n",
    "# https://mlflow.org/docs/latest/tracking.html#automatic-logging\n",
    "# https://mlflow.org/docs/latest/python_api/mlflow.xgboost.html#mlflow.xgboost.autolog\n",
    "mlflow.xgboost.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b39be1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:17:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\objective\\regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nimz\\.conda\\envs\\zoom\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# create a specific run for our hyperparameters\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # create dataset for XGBoost\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # set our parameters and log them\n",
    "    best_params = {\n",
    "        'learning_rate': best_result['learning_rate'],\n",
    "        'max_depth': int(best_result['max_depth']),\n",
    "        'min_child_weight': best_result['min_child_weight'],\n",
    "        'reg_alpha': best_result['reg_alpha'],\n",
    "        'reg_lambda': best_result['reg_lambda'],\n",
    "        'objective': 'reg:linear',  # manually set\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    # train and create an XGBoost model with these parameters\n",
    "    booster = xgb.train(\n",
    "        params=best_params,\n",
    "        dtrain=train,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(valid, 'validation')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False  # suppress output\n",
    "    )\n",
    "\n",
    "    # make predictions, then calculate and log the error\n",
    "    y_pred = booster.predict(valid)\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    mlflow.log_metric('rmse', rmse)\n",
    "    \n",
    "    # create model directory if it doesn't already exist\n",
    "    os.makedirs(os.path.dirname('./models/'), mode=0o755, exist_ok=True)\n",
    "\n",
    "    # save this the preprocessor to our best model locally\n",
    "    with open('./models/preprocessor.b', 'wb') as f_out:\n",
    "        pickle.dump(dv, f_out)\n",
    "    \n",
    "    # log this preprocessor as an experiment artifact\n",
    "    mlflow.log_artifact('models/preprocessor.b', artifact_path='preprocessor')\n",
    "    \n",
    "    # log the actual mode itself\n",
    "    mlflow.xgboost.log_model(booster, artifact_path='models_mlflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a5013",
   "metadata": {},
   "source": [
    "## 3. MLFLow Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97e9580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/17 19:25:05 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.3.2, required: mlflow==2.3)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:25:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\objective\\regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "logged_model = mlflow_model_uri\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b33b56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlflow.pyfunc.loaded_model:\n",
       "  artifact_path: models_mlflow\n",
       "  flavor: mlflow.xgboost\n",
       "  run_id: d4535c4d635f4305940cfbd71b0eb91a"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # check model\n",
    "# loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53192d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:28:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\objective\\regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x1eaee78ab50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can load model in 2 different places as different objects\n",
    "\n",
    "# load model as xgboost object\n",
    "xgboost_model = mlflow.xgboost.load_model(logged_model)\n",
    "xgboost_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf1a54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we have access to attributes\n",
    "# dir(xgboost_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2f34eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.17295 ,  7.193073, 13.691938, 24.373695,  9.104835, 17.14612 ,\n",
       "       11.797985,  9.013244,  8.948696, 19.844584], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions with this loaded model\n",
    "y_pred = xgboost_model.predict(valid)\n",
    "\n",
    "# check 1st 10 predictions\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d65467b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.296879486713688"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred, squared=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-zoom]",
   "language": "python",
   "name": "conda-env-.conda-zoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
